{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Word-level entailment with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Data](#Data)\n",
    "  1. [Edge disjoint](#Edge-disjoint)\n",
    "  1. [Word disjoint](#Word-disjoint)\n",
    "1. [Baseline](#Baseline)\n",
    "  1. [Representing words: vector_func](#Representing-words:-vector_func)\n",
    "  1. [Combining words into inputs: vector_combo_func](#Combining-words-into-inputs:-vector_combo_func)\n",
    "  1. [Classifier model](#Classifier-model)\n",
    "  1. [Baseline results](#Baseline-results)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Hypothesis-only baseline [2 points]](#Hypothesis-only-baseline-[2-points])\n",
    "  1. [Alternatives to concatenation [1 point]](#Alternatives-to-concatenation-[1-point])\n",
    "  1. [A deeper network [2 points]](#A-deeper-network-[2-points])\n",
    "  1. [Your original system [4 points]](#Your-original-system-[4-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general problem is word-level natural language inference.\n",
    "\n",
    "Training examples are pairs of words $(w_{L}, w_{R}), y$ with $y = 1$ if $w_{L}$ entails $w_{R}$, otherwise $0$.\n",
    "\n",
    "The homework questions below ask you to define baseline models for this and develop your own system for entry in the bake-off, which will take place on a held-out test-set distributed at the start of the bake-off. (Thus, all the data you have available for development is available for training your final system before the bake-off begins.)\n",
    "\n",
    "<img src=\"fig/wordentail-diagram.png\" width=600 alt=\"wordentail-diagram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the first notebook in this unit](nli_01_task_and_data.ipynb) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import nli\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = 'data'\n",
    "\n",
    "NLIDATA_HOME = os.path.join(DATA_HOME, 'nlidata')\n",
    "\n",
    "wordentail_filename = os.path.join(\n",
    "    NLIDATA_HOME, 'nli_wordentail_bakeoff_data.json')\n",
    "\n",
    "GLOVE_HOME = os.path.join(DATA_HOME, 'glove.6B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I've processed the data into two different train/test splits, in an effort to put some pressure on our models to actually learn these semantic relations, as opposed to exploiting regularities in the sample.\n",
    "\n",
    "* `edge_disjoint`: The `train` and `dev` __edge__ sets are disjoint, but many __words__ appear in both `train` and `dev`.\n",
    "* `word_disjoint`: The `train` and `dev` __vocabularies are disjoint__, and thus the edges are disjoint as well.\n",
    "\n",
    "These are very different problems. For `word_disjoint`, there is real pressure on the model to learn abstract relationships, as opposed to memorizing properties of individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wordentail_filename) as f:\n",
    "    wordentail_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer keys are the  splits plus a list giving the vocabulary for the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_disjoint', 'vocab', 'word_disjoint'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev', 'train'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data['edge_disjoint'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the split looks like; all three have this same format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['sweater', 'stroke'], 0],\n",
       " [['constipation', 'hypovolemia'], 0],\n",
       " [['disease', 'inflammation'], 0],\n",
       " [['herring', 'animal'], 1],\n",
       " [['cauliflower', 'outlook'], 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data['edge_disjoint']['dev'][: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to make sure no edges are shared between `train` and `dev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_edge_overlap_size(wordentail_data, 'edge_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expect, a *lot* of vocabulary items are shared between `train` and `dev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2916"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_vocab_overlap_size(wordentail_data, 'edge_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large percentage of the entire vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8470"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordentail_data['vocab'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the distribution of labels in the `train` set. It's highly imbalanced, which will pose a challenge for learning. (I'll go ahead and reveal that the `dev` set is similarly distributed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(split):\n",
    "    return pd.DataFrame(wordentail_data[split]['train'])[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14650\n",
       "1     2745\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution('edge_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev', 'train'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordentail_data['word_disjoint'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `word_disjoint` split, no __words__ are shared between `train` and `dev`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_vocab_overlap_size(wordentail_data, 'word_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because no words are shared between `train` and `dev`, no edges are either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli.get_edge_overlap_size(wordentail_data, 'word_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label distribution is similar to that of `edge_disjoint`, though the overall number of examples is a bit smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7199\n",
       "1    1349\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_distribution('word_disjoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in deep learning, __feature representation is vital and requires care!__ For our task, feature representation has two parts: representing the individual words and combining those representations into a single network input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing words: vector_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider two baseline word representations methods:\n",
    "\n",
    "1. Random vectors (as returned by `utils.randvec`).\n",
    "1. 50-dimensional GloVe representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randvec(w, n=50, lower=-1.0, upper=1.0):\n",
    "    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n",
    "    return utils.randvec(n=n, lower=lower, upper=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any of the files in glove.6B will work here:\n",
    "\n",
    "glove_dim = 50\n",
    "\n",
    "glove_src = os.path.join(GLOVE_HOME, 'glove.6B.{}d.txt'.format(glove_dim))\n",
    "\n",
    "# Creates a dict mapping strings (words) to GloVe vectors:\n",
    "GLOVE = utils.glove2dict(glove_src)\n",
    "\n",
    "def glove_vec(w):    \n",
    "    \"\"\"Return `w`'s GloVe representation if available, else return \n",
    "    a random vector.\"\"\"\n",
    "    return GLOVE.get(w, randvec(w, n=glove_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining words into inputs: vector_combo_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decide how to combine the two word vectors into a single representation. In more detail, where `u` is a vector representation of the left word and `v` is a vector representation of the right word, we need a function `vector_combo_func` such that `vector_combo_func(u, v)` returns a new input vector `z` of dimension `m`. A simple example is concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_concatenate(u, v):\n",
    "    \"\"\"Concatenate np.array instances `u` and `v` into a new np.array\"\"\"\n",
    "    return np.concatenate((u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vector_combo_func` could instead be vector average, vector difference, etc. (even combinations of those) â€“ there's lots of space for experimentation here; [homework question 2](#Alternatives-to-concatenation-[1-point]) below pushes you to do some exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier model\n",
    "\n",
    "For a baseline model, I chose `TorchShallowNeuralClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchShallowNeuralClassifier(hidden_dim=50, max_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline results\n",
    "\n",
    "The following puts the above pieces together, using `vector_func=glove_vec`, since `vector_func=randvec` seems so hopelessly misguided for `word_disjoint`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.022924087708815932"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      1910\n",
      "           1       0.41      0.36      0.38       239\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      2149\n",
      "   macro avg       0.66      0.65      0.65      2149\n",
      "weighted avg       0.86      0.87      0.87      2149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_disjoint_experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['word_disjoint']['train'],\n",
    "    assess_data=wordentail_data['word_disjoint']['dev'], \n",
    "    model=net, \n",
    "    vector_func=glove_vec,\n",
    "    vector_combo_func=vec_concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis-only baseline [2 points]\n",
    "\n",
    "During our discussion of SNLI and MultiNLI, we noted that a number of research teams have shown that hypothesis-only baselines for inference tasks can be remarkably robust. This question asks you to explore briefly how this baseline effects the 'edge_disjoint' and 'word_disjoint' versions of our task.\n",
    "\n",
    "For this problem, submit code the following:\n",
    "\n",
    "1. A `vector_combo_func` function called `hypothesis_only` that simply throws away the premise, using the unmodified hypothesis (second) vector as its representation of the example.\n",
    "\n",
    "1. Code for looping over the two conditions 'word_disjoint' and 'edge_disjoint' and the two `vector_combo_func` values `vec_concatenate` and `hypothesis_only`, calling `nli.wordentail_experiment` to train on the conditions 'train' portion and assess on its 'dev' portion, with `glove50vec` as the `vector_func`. So that the results are consistent, use an `sklearn.linear_model.LogisticRegression` with default parameters as the model.\n",
    "\n",
    "1. Print out the percentage-wise increase in macro-F1 over the `hypothesis_only` delivers over `vec_concatenate` for each of the two conditions. For example, if `hypothesis_only` returns 0.5 for condition `C` and  `vec_concatenate` delivers 0.75 for `C`, then you'd report a 50% increase for `C`. The values you need are stored in the dictionary returned by `nli.wordentail_experiment`, with key 'macro-F1'. Please use two digits of precision for the increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def hypothesis_only(u, v):\n",
    "    return v\n",
    "\n",
    "conditions = ['word_disjoint', 'edge_disjoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baidi/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      1910\n",
      "           1       0.47      0.14      0.22       239\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2149\n",
      "   macro avg       0.68      0.56      0.58      2149\n",
      "weighted avg       0.85      0.89      0.86      2149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baidi/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94      1910\n",
      "           1       0.36      0.05      0.09       239\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2149\n",
      "   macro avg       0.63      0.52      0.51      2149\n",
      "weighted avg       0.83      0.88      0.84      2149\n",
      "\n",
      "word_disjoint increase0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baidi/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92      7376\n",
      "           1       0.58      0.23      0.33      1321\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      8697\n",
      "   macro avg       0.73      0.60      0.62      8697\n",
      "weighted avg       0.83      0.86      0.83      8697\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baidi/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      7376\n",
      "           1       0.59      0.19      0.29      1321\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      8697\n",
      "   macro avg       0.73      0.59      0.61      8697\n",
      "weighted avg       0.83      0.86      0.83      8697\n",
      "\n",
      "edge_disjoint increase0.03\n"
     ]
    }
   ],
   "source": [
    "for c in conditions:\n",
    "    concat_experiment = nli.wordentail_experiment(\n",
    "        train_data=wordentail_data[c]['train'],\n",
    "        assess_data=wordentail_data[c]['dev'], \n",
    "        model=LogisticRegression(), \n",
    "        vector_func=glove_vec,\n",
    "        vector_combo_func=vec_concatenate)\n",
    "    hypo_experiment = nli.wordentail_experiment(\n",
    "        train_data=wordentail_data[c]['train'],\n",
    "        assess_data=wordentail_data[c]['dev'], \n",
    "        model=LogisticRegression(), \n",
    "        vector_func=glove_vec,\n",
    "        vector_combo_func=hypothesis_only)\n",
    "    print(c + \" increase{0:.2f}\".format((concat_experiment['macro-F1'] - hypo_experiment['macro-F1']) / hypo_experiment['macro-F1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives to concatenation [1 point]\n",
    "\n",
    "We've so far just used vector concatenation to represent the premise and hypothesis words. This question asks you to explore a simple alternative. \n",
    "\n",
    "For this problem, submit code the following:\n",
    "\n",
    "1. A new potential value for `vector_combo_func` that does something different from concatenation. Options include, but are not limited to, element-wise addition, difference, and multiplication. These can be combined with concatenation if you like.\n",
    "1. Include a use of `nli.wordentail_experiment` in the same configuration as the one in [Baseline results](#Baseline-results) above, but with your new value of `vector_combo_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_add(u, v):\n",
    "    #con = np.concatenate((u, v))\n",
    "    #return np.concatenate((con, np.multiply(u, v)))\n",
    "    return [x + y for x, y in zip(u, v)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 0.9082470536231995"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91      1910\n",
      "           1       0.29      0.31      0.30       239\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      2149\n",
      "   macro avg       0.60      0.61      0.60      2149\n",
      "weighted avg       0.84      0.84      0.84      2149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_disjoint_experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['word_disjoint']['train'],\n",
    "    assess_data=wordentail_data['word_disjoint']['dev'], \n",
    "    model=net, \n",
    "    vector_func=glove_vec,\n",
    "    vector_combo_func=vec_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deeper network [2 points]\n",
    "\n",
    "It is very easy to subclass `TorchShallowNeuralClassifier` if all you want to do is change the network graph: all you have to do is write a new `define_graph`. If your graph has new arguments that the user might want to set, then you should also redefine `__init__` so that these values are accepted and set as attributes.\n",
    "\n",
    "For this question, please subclass `TorchShallowNeuralClassifier` so that it defines the following graph:\n",
    "\n",
    "$$\\begin{align}\n",
    "h_{1} &= xW_{1} + b_{1} \\\\\n",
    "r_{1} &= \\textbf{Bernoulli}(1 - \\textbf{dropout_prob}, n) \\\\\n",
    "d_{1} &= r_1 * h_{1} \\\\\n",
    "h_{2} &= f(d_{1}) \\\\\n",
    "h_{3} &= h_{2}W_{2} + b_{2}\n",
    "\\end{align}$$\n",
    "\n",
    "Here, $r_{1}$ and $d_{1}$ define a dropout layer: $r_{1}$ is a random binary vector of dimension $n$, where the probability of a value being $1$ is given by $1 - \\textbf{dropout_prob}$. $r_{1}$ is multiplied element-wise by our first hidden representation, thereby zeroing out some of the values. The result is fed to the user's activation function $f$, and the result of that is fed through another linear layer to produce $h_{3}$. (Inside `TorchShallowNeuralClassifier`, $h_{3}$ is the basis for a softmax classifier, so no activation function is applied to it.)\n",
    "\n",
    "For comparison, using this notation, `TorchShallowNeuralClassifier` defines the following graph:\n",
    "\n",
    "$$\\begin{align}\n",
    "h_{1} &= xW_{1} + b_{1} \\\\\n",
    "h_{2} &= f(h_{1}) \\\\\n",
    "h_{3} &= h_{2}W_{2} + b_{2}\n",
    "\\end{align}$$\n",
    "\n",
    "The following code starts this sub-class for you, so that you can concentrate on `define_graph`. Be sure to make use of `self.dropout_prob`\n",
    "\n",
    "For this problem, submit just your completed  `TorchDeepNeuralClassifier`. You needn't evaluate it, though we assume you will be keen to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TorchDeepNeuralClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, dropout_prob=0.7, **kwargs):\n",
    "        self.dropout_prob = dropout_prob\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def define_graph(self):\n",
    "        \"\"\"Complete this method!\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        an `nn.Module` instance, which can be a free-standing class you \n",
    "        write yourself, as in `torch_rnn_classifier`, or the outpiut of \n",
    "        `nn.Sequential`, as in `torch_shallow_neural_classifier`.\n",
    "        \n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.Dropout(self.dropout_prob), \n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.n_classes_))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 2.451948955655098"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      1910\n",
      "           1       0.61      0.15      0.25       239\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2149\n",
      "   macro avg       0.75      0.57      0.60      2149\n",
      "weighted avg       0.87      0.89      0.87      2149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['word_disjoint']['train'],\n",
    "    assess_data=wordentail_data['word_disjoint']['dev'], \n",
    "    model=TorchDeepNeuralClassifier(), \n",
    "    vector_func=glove_vec,\n",
    "    vector_combo_func=vec_concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "This is a simple dataset, but our focus on the 'word_disjoint' condition ensures that it's a challenging one, and there are lots of modeling strategies one might adopt. \n",
    "\n",
    "You are free to do whatever you like. We require only that your system differ in some way from those defined in the preceding questions. They don't have to be completely different, though. For example, you might want to stick with the model but represent examples differently, or the reverse.\n",
    "\n",
    "Keep in mind that, for the bake-off evaluation, the 'edge_disjoint' portions of the data are off limits. You can, though, train on the combination of the 'word_disjoint' 'train' and 'dev' portions. You are free to use different pretrained word vectors and the like. Please do not introduce additional entailment datasets into your training data, though.\n",
    "\n",
    "Please embed your code in this notebook so that we can rerun it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "> please install pytorch-pretrained-bert and imblearn before proceeding\n",
    "> you'd need to download our pretrained model at https://drive.google.com/open?id=1THvgq3HdqOZet14IBgbd0wgg515gYS3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "_train_dataset = wordentail_data['word_disjoint']['train']\n",
    "_dev_dataset = wordentail_data['word_disjoint']['dev']\n",
    "\n",
    "## Additional imports\n",
    "import csv, os, json, random, logging, sys, pickle, time\n",
    "from tqdm import trange\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from collections import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Helpers\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "class CondProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(_train_dataset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(_dev_dataset, \"dev\")\n",
    "    \n",
    "    def get_test_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(_test_dataset, \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, data, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "\n",
    "        sents = np.array([d[0] for d in data])\n",
    "        labels = np.array([d[1] for d in data])\n",
    "        examples = []\n",
    "        if set_type == \"train\":\n",
    "            logging.info(\"getting oversampled train data\")\n",
    "            ros = RandomOverSampler(random_state=42, sampling_strategy=0.4)\n",
    "            X_res, y_res = ros.fit_resample(np.arange(len(sents)).reshape(-1, 1), labels)\n",
    "            X_res = sents[X_res.reshape(-1)]\n",
    "\n",
    "            logging.info(Counter(y_res))\n",
    "        else:\n",
    "            logging.info(\"getting dev data\")\n",
    "            X_res = sents\n",
    "            y_res = labels\n",
    "        i = 0\n",
    "        for e, l in zip(X_res, y_res):\n",
    "            text_a = e[0]\n",
    "            text_b = e[1]\n",
    "            label = l\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            i += 1\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "      \n",
    "        return examples\n",
    "\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, show_exp=False):\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[str(example.label)]\n",
    "        if ex_index < 5 and show_exp:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/05/2019 17:00:56 - INFO -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /Users/baidi/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "05/05/2019 17:00:56 - INFO -   loading archive file ./models/\n",
      "05/05/2019 17:00:56 - INFO -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "05/05/2019 17:01:22 - INFO -   getting dev data\n",
      "05/05/2019 17:09:36 - INFO -   Dev F1 0.7580615447598253\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "processor = CondProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "# Prepare model\n",
    "cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_-1')\n",
    "model = BertForSequenceClassification.from_pretrained('./models/', # change this to the dir of pretrained model \n",
    "          cache_dir=cache_dir,\n",
    "          num_labels = num_labels)\n",
    "model.to(device)\n",
    "eval_examples = processor.get_dev_examples()\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, 6, tokenizer)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "preds = []\n",
    "labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    labels.append(label_ids)\n",
    "    preds.append(pred)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    del input_ids, input_mask, segment_ids, label_ids, tmp_eval_loss\n",
    "\n",
    "f1 = f1_score(np.concatenate(labels), np.concatenate(preds), average=\"macro\")\n",
    "logger.info(\"Dev F1 %s\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1-score: 0.7580615447598253\n"
     ]
    }
   ],
   "source": [
    "print(\"macro f1-score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# n_gpu = torch.cuda.device_count()\n",
    "# best_f1 = 0\n",
    "# for learning_rate in [5E-6, 8E-6, 1E-5, 2E-5]:\n",
    "#         args = dotdict({\"data_dir\": './data/bert/', \n",
    "#                 \"bert_model\": \"bert-large-cased\",\n",
    "#                 \"output_dir\": \"./models/\",\n",
    "#                 \"model_save_pth\": \"./models/bert_classification.pth\",\n",
    "#                 \"seed\": 28,\n",
    "#                 \"train_batch_size\": 32,\n",
    "#                 \"num_train_epochs\": 8,\n",
    "#                 \"eval_batch_size\": 128,\n",
    "#                 \"do_lower_case\": False,\n",
    "#                 \"do_train\": True,\n",
    "#                 \"do_eval\": True,\n",
    "#                 \"max_seq_length\": 6,\n",
    "#                 \"gradient_accumulation_steps\": 1,\n",
    "#                 \"task_name\": \"test\",\n",
    "#                 \"local_rank\": -1,\n",
    "#                 \"warmup_proportion\": 0.1,\n",
    "#                 \"fp16\": False,\n",
    "#                 \"cache_dir\": \"./tmp/\",\n",
    "#                 \"learning_rate\": learning_rate})\n",
    "\n",
    "\n",
    "\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "#         logging.info(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         processor = CondProcessor()\n",
    "#         label_list = processor.get_labels()\n",
    "\n",
    "#         tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "#         num_labels = 2\n",
    "#         train_examples = None\n",
    "#         num_train_optimization_steps = None\n",
    "#         if args.do_train:\n",
    "#             train_examples = processor.get_train_examples()\n",
    "#             num_train_optimization_steps = int(\n",
    "#                 len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "#             if args.local_rank != -1:\n",
    "#                 num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "#         # Prepare model\n",
    "#         cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "#         model = BertForSequenceClassification.from_pretrained(args.bert_model,\n",
    "#                   cache_dir=cache_dir,\n",
    "#                   num_labels = num_labels)\n",
    "# #         for i in model.bert.embeddings.parameters():\n",
    "# #             i.requires_grad = False\n",
    "        \n",
    "#         if n_gpu > 1:\n",
    "#             model = torch.nn.DataParallel(model)\n",
    "#         model.to(device)\n",
    "\n",
    "\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#             {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#             ]\n",
    "\n",
    "\n",
    "#         optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                              lr=args.learning_rate,\n",
    "#                              warmup=args.warmup_proportion,\n",
    "#                              t_total=num_train_optimization_steps)\n",
    "        \n",
    "#         train_f1s = []\n",
    "#         eval_f1s = []\n",
    "#         global_step = 0\n",
    "#         nb_tr_steps = 0\n",
    "#         tr_loss = 0\n",
    "#         patience = 0\n",
    "#         if args.do_train:\n",
    "\n",
    "\n",
    "#             for _ in trange(int(args.num_train_epochs)):\n",
    "\n",
    "#                 logger.info(\"do train\")\n",
    "\n",
    "#                 train_features = convert_examples_to_features(\n",
    "#                 train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "#                 all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "#                 all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "#                 all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "#                 all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "#                 train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#                 if args.local_rank == -1:\n",
    "#                     train_sampler = RandomSampler(train_data)\n",
    "#                 else:\n",
    "#                     train_sampler = DistributedSampler(train_data)\n",
    "#                 train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last = True, pin_memory=True)\n",
    "\n",
    "\n",
    "#                 model.train()\n",
    "#                 tr_loss = 0\n",
    "#                 nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "#                 for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "\n",
    "#                     batch = tuple(t.to(device, non_blocking=True) for t in batch)\n",
    "\n",
    "#                     input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "#                     loss = model(input_ids, segment_ids, input_mask, label_ids) #???\n",
    "#                     if n_gpu > 1:\n",
    "#                         loss = loss.mean() # mean() to average on multi-gpu.\n",
    "#                     loss.backward()\n",
    "\n",
    "#                     tr_loss += loss.item()\n",
    "#                     nb_tr_examples += input_ids.size(0)\n",
    "#                     nb_tr_steps += 1\n",
    "#                     optimizer.step()\n",
    "#                     optimizer.zero_grad()\n",
    "#                     global_step += 1\n",
    "#                     del input_ids, input_mask, segment_ids, label_ids, batch, loss\n",
    "\n",
    "#                 logger.info(\"tr loss: %s\" % tr_loss)\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "#                 if args.do_eval:\n",
    "#                     logger.info(\"do eval\")\n",
    "\n",
    "#                     eval_examples = processor.get_dev_examples()\n",
    "#                     eval_features = convert_examples_to_features(\n",
    "#                         eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "#                     all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "#                     all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "#                     all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "#                     all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "#                     eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#                     # Run prediction for full data\n",
    "#                     eval_sampler = SequentialSampler(eval_data)\n",
    "#                     eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "#                     model.eval()\n",
    "#                     eval_loss = 0\n",
    "#                     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "#                     preds = []\n",
    "#                     labels = []\n",
    "#                     for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "#                         input_ids = input_ids.to(device)\n",
    "#                         input_mask = input_mask.to(device)\n",
    "#                         segment_ids = segment_ids.to(device)\n",
    "#                         label_ids = label_ids.to(device)\n",
    "\n",
    "#                         with torch.no_grad():\n",
    "#                             tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#                             logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "#                         logits = logits.detach().cpu().numpy()\n",
    "#                         label_ids = label_ids.to('cpu').numpy()\n",
    "#                         pred = np.argmax(logits, axis=1)\n",
    "#                         labels.append(label_ids)\n",
    "#                         preds.append(pred)\n",
    "#                         eval_loss += tmp_eval_loss.mean().item()\n",
    "\n",
    "#                         nb_eval_examples += input_ids.size(0)\n",
    "#                         nb_eval_steps += 1\n",
    "#                         del input_ids, input_mask, segment_ids, label_ids, tmp_eval_loss\n",
    "\n",
    "#                     f1 = f1_score(np.concatenate(labels), np.concatenate(preds), average=\"macro\")\n",
    "#                     logger.info(\"F1 %s\" % f1)\n",
    "#                     eval_f1s.append(f1)\n",
    "#                     if f1 > best_f1:\n",
    "\n",
    "#                         best_f1 = f1\n",
    "#                         assert type(eval_loss) == float\n",
    "#                         logger.info(\"Best F1 %s\" % best_f1)\n",
    "#                         result = {'eval_loss': eval_loss,\n",
    "#                                   'eval_f1': f1,\n",
    "#                                   'global_step': global_step}\n",
    "\n",
    "\n",
    "#                         model_to_save = model.module if hasattr(model, 'module') else model \n",
    "#                         output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n",
    "#                         torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#                         output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n",
    "#                         with open(output_config_file, 'w') as f:\n",
    "#                             f.write(model_to_save.config.to_json_string())\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "The goal of the bake-off is to achieve the highest macro-average F1 score on __word_disjoint__, on a test set that we will make available at the start of the bake-off on May 6. The announcement will go out on Piazza. To enter, you'll be asked to run `nli.bake_off_evaluation` on the output of your chosen `nli.wordentail_experiment` run. \n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187250\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "The rules described in the [Your original system](#Your-original-system-[4-points]) homework question are also in effect for the bake-off.\n",
    "\n",
    "Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on May 8. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bake-off assessment code into this cell. \n",
    "# Please do not remove this comment.\n",
    "test_data_filename = os.path.join(\n",
    "            'data', 'nlidata', 'nli_wordentail_bakeoff_data-test.json')\n",
    "with open(test_data_filename) as f:\n",
    "    wordentail_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please install pytorch-pretrained-bert and imblearn before proceeding\n",
    "# you'd need to download our pretrained model at https://drive.google.com/open?id=1THvgq3HdqOZet14IBgbd0wgg515gYS3s\n",
    "\n",
    "_test_dataset = wordentail_data['word_disjoint']['test']\n",
    "\n",
    "## Additional imports\n",
    "import csv, os, json, random, logging, sys, pickle, time\n",
    "from tqdm import trange\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from collections import *\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## Helpers\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "class CondProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(_train_dataset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(_dev_dataset, \"dev\")\n",
    "    \n",
    "    def get_test_examples(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(_test_dataset, \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, data, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "\n",
    "        sents = np.array([d[0] for d in data])\n",
    "        labels = np.array([d[1] for d in data])\n",
    "        examples = []\n",
    "        if set_type == \"train\":\n",
    "            logging.info(\"getting oversampled train data\")\n",
    "            ros = RandomOverSampler(random_state=42, sampling_strategy=0.4)\n",
    "            X_res, y_res = ros.fit_resample(np.arange(len(sents)).reshape(-1, 1), labels)\n",
    "            X_res = sents[X_res.reshape(-1)]\n",
    "\n",
    "            logging.info(Counter(y_res))\n",
    "        else:\n",
    "            X_res = sents\n",
    "            y_res = labels\n",
    "        i = 0\n",
    "        for e, l in zip(X_res, y_res):\n",
    "            text_a = e[0]\n",
    "            text_b = e[1]\n",
    "            label = l\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            i += 1\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "      \n",
    "        return examples\n",
    "\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, show_exp=False):\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[str(example.label)]\n",
    "        if ex_index < 5 and show_exp:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2019 15:38:45 - INFO -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /Users/baidi/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "05/06/2019 15:38:46 - INFO -   loading archive file ./models/\n",
      "05/06/2019 15:38:46 - INFO -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7581151196b7481eb9b06fa36c25db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=70), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2019 15:42:28 - INFO -   test: macro-F1 0.7825426994039539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "processor = CondProcessor()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "# Prepare model\n",
    "cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_-1')\n",
    "model = BertForSequenceClassification.from_pretrained('./models/', # change this to the dir of pretrained model \n",
    "          cache_dir=cache_dir,\n",
    "          num_labels = num_labels)\n",
    "model.to(device)\n",
    "eval_examples = processor.get_test_examples()\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, 6, tokenizer)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "preds = []\n",
    "labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    labels.append(label_ids)\n",
    "    preds.append(pred)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    del input_ids, input_mask, segment_ids, label_ids, tmp_eval_loss\n",
    "\n",
    "f1 = f1_score(np.concatenate(labels), np.concatenate(preds), average=\"macro\")\n",
    "logger.info(\"test: macro-F1 %s\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-avg f1 value as reported by the code above. \n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "0.7825426994039539"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
